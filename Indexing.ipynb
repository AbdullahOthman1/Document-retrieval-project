{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from geopy.geocoders import Nominatim\n",
    "import spacy\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    [{'host': 'localhost', 'port': 9200, 'scheme': 'http'}],\n",
    "    http_auth=('Abdullah', 'Abdullah')\n",
    ")\n",
    "\n",
    "index_name = \"reuters_news_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "\n",
    "configurations = {\n",
    "    \"mappings\": {\n",
    "        \"dynamic_templates\": [\n",
    "            {\n",
    "                \"dates_template\": {\n",
    "                    \"match\": \"TemporalExpressions.*\",\n",
    "                    \"mapping\": {\n",
    "                        \"type\": \"date\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"geopoints_template\": {\n",
    "                    \"match\": \"Georeferences.*\",\n",
    "                    \"mapping\": {\n",
    "                        \"type\": \"geo_point\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"properties\": {\n",
    "            \"Title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"autocomplete_analyzer\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"Content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"content_analyzer\",\n",
    "            },\n",
    "            \"Authors\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"first_name\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"standard\"\n",
    "                    },\n",
    "                    \"last_name\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"standard\"\n",
    "                    },\n",
    "                    \"email\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"Date\": {\n",
    "                \"type\": \"date\"\n",
    "            },\n",
    "            \"Geopoints\": {\n",
    "                \"type\": \"geo_point\"\n",
    "            },\n",
    "            \"TemporalExpressions\": {\n",
    "                \"type\": \"nested\"\n",
    "            },\n",
    "            \"Georeferences\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 1,\n",
    "        \"analysis\": {\n",
    "           \"analyzer\": {\n",
    "                \"autocomplete_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"autocomplete_filter\",\n",
    "                        \"fuzzy_filter\"\n",
    "                    ]\n",
    "                },\n",
    "                \"content_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"stop_filter\",\n",
    "                        \"stemmer_filter\"\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"autocomplete_filter\": {\n",
    "                    \"type\": \"edge_ngram\",\n",
    "                    \"min_gram\": 1,\n",
    "                    \"max_gram\": 20\n",
    "                },\n",
    "                \"stop_filter\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords\": \"_english_\"\n",
    "                },\n",
    "                \"stemmer_filter\": {\n",
    "                    \"type\": \"porter_stem\"\n",
    "                },\n",
    "                \"fuzzy_filter\": {\n",
    "                    \"type\": \"fuzzy\",\n",
    "                    \"fuzziness\": \"2\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.indices.create(index=index_name, ignore=400, body=configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "geolocator = Nominatim(user_agent=\"geo_app\", timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content(content):\n",
    "    clean_content = re.sub(r'<[^>]+>', '', content)\n",
    "    tokens = word_tokenize(clean_content.lower())\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    processed_text = ' '.join(stemmer.stem(word) for word in tokens if word not in stop_words and len(word) >= 3)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_author_info(author_tag):\n",
    "    if not author_tag:\n",
    "        return None\n",
    "\n",
    "    author_text = author_tag.get_text().strip().replace('By ', '').replace('by ', '')\n",
    "    authors = [author.strip().split() for author in author_text.split(',') if author.strip()]\n",
    "\n",
    "    authors_info = [\n",
    "        {\n",
    "            \"first_name\": authors[0][0].strip(),\n",
    "            \"last_name\": ' '.join(authors[0][1:]).strip() if len(authors) > 1 else None,\n",
    "            \"email\": None\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return authors_info if authors_info else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(date_tags):\n",
    "    if date_tags and isinstance(date_tags, list):\n",
    "        date_string = date_tags[0].text.strip()\n",
    "\n",
    "        try:\n",
    "            return datetime.strptime(date_string, \"%d-%b-%Y %H:%M:%S.%f\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: Unable to parse date string '{date_string}': {e}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts_in_tags(soup, es, index_name, geolocator, nlp):\n",
    "    for reuters_tag in soup.find_all('reuters'):\n",
    "        title = reuters_tag.find('title').get_text() if reuters_tag.find('title') else None\n",
    "        date = convert_date([reuters_tag.find('date')]) if reuters_tag.find('date') else None\n",
    "        places = [place.get_text() for place in reuters_tag.find('places').find_all('d')] if reuters_tag.find('places') else None\n",
    "        author_tag = reuters_tag.find('author')\n",
    "        author_info = extract_author_info(author_tag) if author_tag else None\n",
    "        content = reuters_tag.find('text').get_text() if reuters_tag.find('text') else None\n",
    "        content = clean_content(content) if content else None\n",
    "\n",
    "        georeferences = []\n",
    "        coordinates = []\n",
    "        for place_name in places or []:\n",
    "            location = geolocator.geocode(place_name)\n",
    "            if location:\n",
    "                georeferences.append({'name': place_name})\n",
    "                coordinates.append({'latitude': location.latitude, 'longitude': location.longitude})\n",
    "\n",
    "        average_location = {\n",
    "            \"lat\": np.mean([point[\"latitude\"] for point in coordinates]),\n",
    "            \"lon\": np.mean([point[\"longitude\"] for point in coordinates])\n",
    "        } if coordinates else None\n",
    "\n",
    "        temporal_expressions = []\n",
    "        if content:\n",
    "            doc = nlp(content)\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'DATE':\n",
    "                    nested_entity = {\"text\": ent.text, \"start\": ent.start_char, \"end\": ent.end_char}\n",
    "                    temporal_expressions.append(nested_entity)\n",
    "\n",
    "        document_dict = {\n",
    "            \"Title\": title,\n",
    "            \"Content\": content,\n",
    "            \"Authors\": author_info,\n",
    "            \"Date\": date,\n",
    "            \"Georeferences\": georeferences,\n",
    "            \"Geopoints\": average_location,\n",
    "            \"TemporalExpressions\": temporal_expressions\n",
    "        }\n",
    "        \n",
    "        es.index(index=index_name, body=document_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "def extract_texts_in_tags_bulk(soup, es, index_name, geolocator, nlp):\n",
    "    actions = []\n",
    "\n",
    "    for reuters_tag in soup.find_all('reuters'):\n",
    "        title = reuters_tag.find('title').get_text() if reuters_tag.find('title') else None\n",
    "        date = convert_date([reuters_tag.find('date')]) if reuters_tag.find('date') else None\n",
    "        places = [place.get_text() for place in reuters_tag.find('places').find_all('d')] if reuters_tag.find('places') else None\n",
    "        author_tag = reuters_tag.find('author')\n",
    "        author_info = extract_author_info(author_tag) if author_tag else None\n",
    "        content = reuters_tag.find('text').get_text() if reuters_tag.find('text') else None\n",
    "        content = clean_content(content) if content else None\n",
    "\n",
    "        georeferences = []\n",
    "        coordinates = []\n",
    "        for place_name in places or []:\n",
    "            location = geolocator.geocode(place_name)\n",
    "            if location:\n",
    "                georeferences.append(place_name)\n",
    "                coordinates.append({'latitude': location.latitude, 'longitude': location.longitude})\n",
    "\n",
    "        average_location = {\n",
    "            \"lat\": np.mean([point[\"latitude\"] for point in coordinates]),\n",
    "            \"lon\": np.mean([point[\"longitude\"] for point in coordinates])\n",
    "        } if coordinates else None\n",
    "\n",
    "        temporal_expressions = []\n",
    "        if content:\n",
    "            doc = nlp(content)\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'DATE':\n",
    "                    nested_entity = {\"text\": ent.text, \"start\": ent.start_char, \"end\": ent.end_char}\n",
    "                    temporal_expressions.append(nested_entity)\n",
    "\n",
    "        document_dict = {\n",
    "            \"_op_type\": \"index\",\n",
    "            \"_index\": index_name,\n",
    "            \"_source\": {\n",
    "                \"Title\": title,\n",
    "                \"Content\": content,\n",
    "                \"Authors\": author_info,\n",
    "                \"Date\": date,\n",
    "                \"Georeferences\": georeferences,\n",
    "                \"Geopoints\": average_location,\n",
    "                \"TemporalExpressions\": temporal_expressions\n",
    "            }\n",
    "        }\n",
    "\n",
    "        actions.append(document_dict)\n",
    "\n",
    "    success, failed = bulk(es, actions)\n",
    "\n",
    "    print(f\"Successfully indexed {success} documents\")\n",
    "    if failed:\n",
    "        print(f\"Failed to index {failed} documents\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = Path('./Data')\n",
    "\n",
    "for file_path in directory_path.glob('*.sgm'):\n",
    "    with file_path.open('r', encoding='utf-8', errors='ignore') as file:\n",
    "        sgm_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(sgm_content, 'html.parser')\n",
    "    #extract_texts_in_tags(soup ,es, index_name, geolocator, nlp)\n",
    "    extract_texts_in_tags_bulk(soup, es, index_name, geolocator, nlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
