{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from geopy.geocoders import Nominatim\n",
    "import spacy\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdullah\\AppData\\Local\\Temp\\ipykernel_19600\\2892260069.py:1: DeprecationWarning: The 'http_auth' parameter is deprecated. Use 'basic_auth' or 'bearer_auth' parameters instead\n",
      "  es = Elasticsearch(\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch(\n",
    "    [{'host': 'localhost', 'port': 9200, 'scheme': 'http'}],\n",
    "    http_auth=('Abdullah', 'Abdullah')\n",
    ")\n",
    "\n",
    "index_name = \"reuters_news_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdullah\\AppData\\Local\\Temp\\ipykernel_19600\\2576661828.py:89: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es.indices.create(index=index_name, ignore=400, body=configurations)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'error': {'root_cause': [{'type': 'mapper_parsing_exception', 'reason': 'Unknown parameter [boost] on mapper [Content]'}], 'type': 'mapper_parsing_exception', 'reason': 'Failed to parse mapping: Unknown parameter [boost] on mapper [Content]', 'caused_by': {'type': 'mapper_parsing_exception', 'reason': 'Unknown parameter [boost] on mapper [Content]'}}, 'status': 400})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "\n",
    "configurations = {\n",
    "    \"mappings\": {\n",
    "        \"dynamic_templates\": [\n",
    "            {\n",
    "                \"dates_template\": {\n",
    "                    \"match\": \"temporalExpressions\",\n",
    "                    \"mapping\": {\n",
    "                        \"type\": \"date\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"geopoints_template\": {\n",
    "                    \"match\": \"georeferences\",\n",
    "                    \"mapping\": {\n",
    "                        \"type\": \"geo_point\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"properties\": {\n",
    "            \"Title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"keyword\",\n",
    "                        \"ignore_above\": 256\n",
    "                    }\n",
    "                },\n",
    "                \"analyzer\": \"autocomplete_analyzer\",\n",
    "                \"boost\": 2\n",
    "            },\n",
    "            \"Content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"custom_content_analyzer\",\n",
    "                \"boost\": 1\n",
    "            },\n",
    "            \"Authors\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"first_name\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"standard\"\n",
    "                    },\n",
    "                    \"last_name\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"analyzer\": \"standard\"\n",
    "                    },\n",
    "                    \"email\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"autocomplete_analyzer\": {\n",
    "                    \"tokenizer\": \"autocomplete_tokenizer\",\n",
    "                    \"filter\": [\"lowercase\"]\n",
    "                },\n",
    "                \"custom_content_analyzer\": {\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\"lowercase\", \"stop\", \"custom_length_filter\", \"snowball\"]\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"autocomplete_tokenizer\": {\n",
    "                    \"type\": \"edge_ngram\",\n",
    "                    \"min_gram\": 2,\n",
    "                    \"max_gram\": 10,\n",
    "                    \"token_chars\": [\"letter\", \"digit\"]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"custom_length_filter\": {\n",
    "                    \"type\": \"length\",\n",
    "                    \"min\": 3\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.create(index=index_name, ignore=400, body=configurations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "geolocator = Nominatim(user_agent=\"geo_app\", timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content(content):\n",
    "    clean_content = re.sub(r'<[^>]+>', '', content)\n",
    "    tokens = word_tokenize(clean_content.lower())\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    processed_text = ' '.join(stemmer.stem(word) for word in tokens if word not in stop_words and len(word) >= 3)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_author_info(author_tag):\n",
    "    if not author_tag:\n",
    "        return None\n",
    "\n",
    "    author_text = author_tag.get_text().strip().replace('by ', '')\n",
    "    authors = [author.strip().split() for author in author_text.split(',') if author.strip()]\n",
    "\n",
    "    authors_info = [\n",
    "        {\n",
    "            \"first_name\": parts[0].strip(),\n",
    "            \"last_name\": ' '.join(parts[1:]).strip() if len(parts) > 1 else None,\n",
    "            \"email\": None\n",
    "        }\n",
    "        for parts in authors\n",
    "    ]\n",
    "\n",
    "    return authors_info if authors_info else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(date_tags):\n",
    "    if date_tags and isinstance(date_tags, list):\n",
    "        date_string = date_tags[0].text.strip()\n",
    "\n",
    "        try:\n",
    "            return datetime.strptime(date_string, \"%d-%b-%Y %H:%M:%S.%f\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error: Unable to parse date string '{date_string}': {e}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reuters_tags(soup):\n",
    "    for reuters_tag in soup.find_all('reuters'):\n",
    "        date = convert_date([reuters_tag.find('date')]) if reuters_tag.find('date') else None\n",
    "        topics = [topic.get_text() for topic in reuters_tag.find('topics').find_all('d')] if reuters_tag.find('topics') else None\n",
    "        places = [place.get_text() for place in reuters_tag.find('places').find_all('d')] if reuters_tag.find('places') else None\n",
    "        title = reuters_tag.find('title').get_text() if reuters_tag.find('title') else None\n",
    "        author_tag = reuters_tag.find('author')\n",
    "        author_info = extract_author_info(author_tag) if author_tag else None\n",
    "        content = reuters_tag.find('text').get_text() if reuters_tag.find('text') else None\n",
    "        content = clean_content(content) if content else None\n",
    "\n",
    "        georeferences = []\n",
    "        coordinates = []\n",
    "        for place_name in places or []:\n",
    "            location = geolocator.geocode(place_name)\n",
    "            if location:\n",
    "                georeferences.append(place_name)\n",
    "                coordinates.append({'latitude': location.latitude, 'longitude': location.longitude})\n",
    "\n",
    "        temporal_expressions = []\n",
    "        if content:\n",
    "            doc = nlp(content)\n",
    "            temporal_expressions = [ent.text for ent in doc.ents if ent.label_ == 'DATE']\n",
    "\n",
    "        document_dict = {\n",
    "            \"Title\": title,\n",
    "            \"Content\": content,\n",
    "            \"Authors\": author_info,\n",
    "            \"Date\": date,\n",
    "            \"Georeferences\": georeferences,\n",
    "            \"Coordinates\": {\"latitude\": coordinates[0][\"latitude\"], \"longitude\": coordinates[0][\"longitude\"]} if coordinates else None,\n",
    "            \"TemporalExpressions\": temporal_expressions,\n",
    "        }\n",
    "\n",
    "        es.index(index=index_name, body=document_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = Path('./Sample_Data')\n",
    "\n",
    "for file_path in directory_path.glob('*.sgm'):\n",
    "    with file_path.open('r', encoding='utf-8', errors='ignore') as file:\n",
    "        sgm_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(sgm_content, 'html.parser')\n",
    "    process_reuters_tags(soup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
